{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"vaalit_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2437 entries, 0 to 2436\n",
      "Columns: 212 entries, vaalipiiri to Lappi. Jäämeren rata pitää rakentaa..1\n",
      "dtypes: float64(5), int64(1), object(206)\n",
      "memory usage: 3.9+ MB\n",
      "None\n",
      "['Lappi. Lappiin ei saa avata yhtään uutta kaivosta ennen kuin yhtiöiltä aletaan periä kaivosveroa.', 'Lappi. Mahdollisen uuden maakuntahallinnon toimintoja ja työpaikkoja on sijoitettava muuallekin kuin Rovaniemelle.', 'Lappi. Lapin erikoissairaanhoito on keskitettävä kokonaan Rovaniemelle Lapin keskussairaalaan.', 'Lappi. Alkuperäiskansojen oikeuksia koskeva ILO-sopimus pitää ratifioida seuraavalla hallituskaudella. ', 'Lappi. Jäämeren rata pitää rakentaa.', 'Lappi. Lappiin ei saa avata yhtään uutta kaivosta ennen kuin yhtiöiltä aletaan periä kaivosveroa..1', 'Lappi. Mahdollisen uuden maakuntahallinnon toimintoja ja työpaikkoja on sijoitettava muuallekin kuin Rovaniemelle..1', 'Lappi. Lapin erikoissairaanhoito on keskitettävä kokonaan Rovaniemelle Lapin keskussairaalaan..1', 'Lappi. Alkuperäiskansojen oikeuksia koskeva ILO-sopimus pitää ratifioida seuraavalla hallituskaudella. .1', 'Lappi. Jäämeren rata pitää rakentaa..1'] 10\n"
     ]
    }
   ],
   "source": [
    "print(all_data.info())\n",
    "\n",
    "# Look at lappi related columns\n",
    "lappi_columns = [col for col in all_data.columns if col[:6] == 'Lappi.']\n",
    "print(lappi_columns, len(lappi_columns))\n",
    "\n",
    "# Print answers to a lappi question. We'll use these to test the encoder.\n",
    "lappi_question_idx = -1\n",
    "lappi_data = all_data[lappi_columns[lappi_question_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nykyinen ratasuunnitelma on todettu tutkimuksissa nykytilanteessa kannattamattomaksi. Lisäksi hanke aiheuttaa ongelmia ympäristölle ja alueen väestölle. Suunnitelmassa on kuitenkin yksi mielenkiintoinen osa. Junaradan jatkaminen Kemijärveltä Pyhätunturin kautta Sodankylään voisi olla toteuttamiskelpoinen ajatus.\n",
      "\n",
      "Ratayhteys Jäämerelle voidaan saada aikaan toisella tavalla varsin helposti ja selvästi halvemmalla. Päivitetään nykyinen  Laurila-Tornio-Haaparanta ratayhteys. Tämän päivityksen hinta on vain murto-osa nyt suunnitellusta linjauksesta ja sillä saavutettaisiin ratayhteys Jäämeren ja Atlantin lisäksi myös keskeiseen Eurooppaan ja vaikka Välimerelle saakka. \n"
     ]
    }
   ],
   "source": [
    "print(lappi_data.iloc[2432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Finnish-NLP/roberta-large-finnish were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at Finnish-NLP/roberta-large-finnish and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at TurkuNLP/bert-large-finnish-cased-v1 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load both turku and roberta models. Turku seems more rigorous and documented.\n",
    "model_id_turku = 'TurkuNLP/bert-large-finnish-cased-v1'\n",
    "model_id_roberta = 'Finnish-NLP/roberta-large-finnish'\n",
    "\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(model_id_roberta)\n",
    "tokenizer_turku = AutoTokenizer.from_pretrained(model_id_turku)\n",
    "\n",
    "model_roberta = AutoModel.from_pretrained(model_id_roberta)\n",
    "model_turku = AutoModel.from_pretrained(model_id_turku)\n",
    "\n",
    "models = [model_roberta, model_turku]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To look at model architectures\n",
    "# models[0]\n",
    "# models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 3.5597, -0.1192, -0.4937,  ...,  0.5554,  0.9808,  0.3700],\n",
      "         [ 4.0428, -0.5044, -0.6349,  ..., -0.0724, -1.4574,  0.8273],\n",
      "         [ 3.6685, -0.2831, -1.1789,  ..., -1.1386, -0.1853,  0.3881],\n",
      "         ...,\n",
      "         [ 3.0214,  1.7404, -0.3069,  ..., -0.1230, -0.4943,  0.1827],\n",
      "         [ 3.9432,  1.9230, -0.7434,  ..., -0.5298,  0.7216,  0.0497],\n",
      "         [-3.4648,  1.1225, -0.6437,  ...,  0.5395, -0.4789,  0.8119]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.0730, -1.0000,  0.9520,  ..., -0.2425, -0.1296, -0.3292]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Test with a single row on dataset.\n",
    "text = lappi_data.iloc[2432]\n",
    "tokenized_input = tokenizer_turku(text, return_tensors='pt')\n",
    "token_encodings = models[1](**tokenized_input)\n",
    "\n",
    "print(token_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 112, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0730, -1.0000,  0.9520,  ..., -0.2425, -0.1296, -0.3292]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pooled output is just the hidden state of the first token passed through a linear layer.\n",
    "# Should we to start with just do avg pooling on all tokens and represent that as a node feature representing a single sentence?\n",
    "\n",
    "# print(token_encodings['input_ids'].shape) # B, T\n",
    "print(token_encodings[0].shape) # B, T, D\n",
    "\n",
    "token_encodings.pooler_output.data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaaliperttu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8aea22ac87b4d7775a71809f35a847f66f81cf5bafffb9ad80f891cde983deb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
