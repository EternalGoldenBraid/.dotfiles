# training:
#   epochs: 300
#   batch_size: 32
#   learning_rate: 0.0001
#   optimizer: "adamw"
#   loss_function: "bcewithlogitsloss"
#   scheduler:
#     type: "cosineannealinglr"
#     lr_min: 1e-6
#     lr_max: 1e-4
#     t_max: 100

epochs: 300
batch_size: 32
learning_rate: 0.0001
optimizer: "adamw"
loss_function: "bcewithlogitsloss"
scheduler:
  type: "cosineannealinglr"
  lr_min: 1e-6
  lr_max: 1e-4
  t_max: 100