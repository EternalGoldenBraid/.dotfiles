training:
  epochs: 300
  batch_size: 32
  learning_rate: 0.0001
  optimizer: "AdamW"
  loss_function: "BCEWithLogitsLoss"
  scheduler:
    type: "CosineAnnealingLR"
    lr_min: 1e-6
    lr_max: 1e-4
    T_max: 100
