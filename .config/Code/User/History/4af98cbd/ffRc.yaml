training:
  epochs: 300
  batch_size: 32
  learning_rate: 0.0001
  optimizer: "AdamW"
  loss_function: "BCEWithLogitsLoss"
  scheduler:
    type: "CosineAnnealingLR"
    T_max: 100
