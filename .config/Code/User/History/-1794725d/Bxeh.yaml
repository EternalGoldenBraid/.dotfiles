# language_encoders:
#   bert:
#     model_name: "bert-base-uncased" # or any other BERT model you want to use
#     max_seq_length: 128
#     freeze: True
#   distilbert:
#     model_name: "distilbert-base-uncased" # or any other DistilBERT model you want to use
#     max_seq_length: 128
#     freeze: True
#   roberta:
#     model_name: "roberta-base" # or any other RoBERTa model you want to use
#     max_seq_length: 128
#     freeze: True
#   xlnet:
#     model_name: "xlnet-base-cased" # or any other XLNet model you want to use
#     max_seq_length: 128
#     freeze: True

bert:
  # model_name: "bert-base-uncased" # or any other BERT model you want to use
  model_name: "bert-large-cased" # or any other BERT model you want to use
  max_seq_length: 512
  # freeze: True
  batch_size: 12
minilm:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  max_seq_length: 512
  batch_size: 42
distilbert:
  model_name: "distilbert-base-uncased" # or any other DistilBERT model you want to use
  max_seq_length: 128
roberta:
  model_name: "roberta-base" # or any other RoBERTa model you want to use
  max_seq_length: 128
  freeze: True
xlnet:
  model_name: "xlnet-base-cased" # or any other XLNet model you want to use
  max_seq_length: 128
  freeze: True